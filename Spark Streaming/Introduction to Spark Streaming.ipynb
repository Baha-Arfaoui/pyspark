{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark Streaming\n",
    "\n",
    "_____\n",
    "### Note on  Streaming\n",
    "Streaming is something that is rapidly advancing and changin fast, there are multipl enew libraries every year, new and different services always popping up, and what is in this notebook may or may not apply to you. Maybe your looking for something specific on Kafka, or maybe you are looking for streaming about twitter, in which case Spark might be overkill for what you really want. Realistically speaking each situation is going to require a customized solution and this course is never going to be able to supply a one size fits all solution. Because of this, I wanted to point out some great resources for Python and Spark StreamingL\n",
    "\n",
    "* [The Official Documentation is great. This should be your first go to.](http://spark.apache.org/docs/latest/streaming-programming-guide.html#spark-streaming-programming-guide)\n",
    "\n",
    "* [Fantastic Guide to Spark Streaming with Kafka](https://www.rittmanmead.com/blog/2017/01/getting-started-with-spark-streaming-with-python-and-kafka/)\n",
    "\n",
    "* [Another Spark Streaming Example with Geo Plotting](http://nbviewer.jupyter.org/github/ibm-cds-labs/spark.samples/blob/master/notebook/DashDB%20Twitter%20Car%202015%20Python%20Notebook.ipynb)\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has pretty well known Streaming Capabilities, if streaming is something you've found yourself needing at work then you are probably familiar with some of these concepts already, in which case you may find it more useful to jump straight to the official documentation here:\n",
    "\n",
    "http://spark.apache.org/docs/latest/streaming-programming-guide.html#spark-streaming-programming-guide\n",
    "\n",
    "It is really a great guide, but keep in mind some of the features are restricted to Scala at this time (Spark 2.1), hopefully they will be expanded to the Python API in the future!\n",
    "\n",
    "For those of you new to Spark Streaming, let's get started with a classic example, streaming Twitter! Twitter is a great source for streaming because its something most people already have an intuitive understanding of, you can visit the site yourself, and a lot of streaming technology has come out of Twitter as a company. You don't access to the entire \"firehose\" of twitter without paying for it, but that would be too much for us to handle anyway, so we'll be more than fine with the freely available API access.\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss SparkStreaming!\n",
    "\n",
    "Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Sparkâ€™s machine learning and graph processing algorithms on data streams.\n",
    "\n",
    "<img src='http://spark.apache.org/docs/latest/img/streaming-arch.png'/>\n",
    "\n",
    "Keep in mind that a few of these Streamiing Capabilities are limited when it comes to Python, you'll need to reference the documentation for the most up to date information. Also the streaming contexts tend to follow more along with the older RDD syntax, so a few things might seem different than what we are used to seeing, keep that in mind, you'll definitely want to have a good understanding of lambda expressions before continuing with this!\n",
    "\n",
    "There are SparkSQL modules for streaming: \n",
    "\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=streaming#module-pyspark.sql.streaming\n",
    "\n",
    "But they are all still listed as experimental, so instead of showing you somethign that might break in the future, we'll stick to the RDD methods (which is what the documentation also currently shows for streaming).\n",
    "\n",
    "Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.\n",
    "\n",
    "<img src='http://spark.apache.org/docs/latest/img/streaming-flow.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Local Example\n",
    "\n",
    "We'll do a simple local counting example, make sure to watch the video for this, the example will only work on Linux type systems, not on a Windows computer. This makes sense because you won't run this on Windows in the real world. Definitely watch the video for this one, a lot of it can't be replicated on Jupyter Notebook by itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BAHA\\anaconda3\\lib\\site-packages\\pyspark\\streaming\\context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "# Firewalls might block this!\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we open up a Unix terminal and type:\n",
    "\n",
    "         $ nc -lk 9999\n",
    "     $ hello world any text you want\n",
    "     \n",
    "With this running run the line below, then type Ctrl+C to terminate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o25.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"c:\\Users\\BAHA\\anaconda3\\lib\\site-packages\\pyspark\\streaming\\util.py\", line 71, in call\n    r = self.func(t, *rdds)\n  File \"c:\\Users\\BAHA\\anaconda3\\lib\\site-packages\\pyspark\\streaming\\dstream.py\", line 254, in takeAndPrint\n    taken = rdd.take(num + 1)\n  File \"c:\\Users\\BAHA\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 2836, in take\n    res = self.context.runJob(self, takeUpToNumLeft, p)\n  File \"c:\\Users\\BAHA\\anaconda3\\lib\\site-packages\\pyspark\\context.py\", line 2319, in runJob\n    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n  File \"c:\\Users\\BAHA\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n    return_value = get_return_value(\n  File \"c:\\Users\\BAHA\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 1) (DESKTOP-4JIO9JB.home executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 18 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\r\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\r\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\r\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\r\n\tat com.sun.proxy.$Proxy33.call(Unknown Source)\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\t... 3 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 18 more\r\n\n\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\BAHA\\OneDrive - Positive Thinking Company\\Bureau\\Python-and-Spark-for-Big-Data-master\\Spark Streaming\\Introduction to Spark Streaming.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/BAHA/OneDrive%20-%20Positive%20Thinking%20Company/Bureau/Python-and-Spark-for-Big-Data-master/Spark%20Streaming/Introduction%20to%20Spark%20Streaming.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ssc\u001b[39m.\u001b[39mstart()             \u001b[39m# Start the computation\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/BAHA/OneDrive%20-%20Positive%20Thinking%20Company/Bureau/Python-and-Spark-for-Big-Data-master/Spark%20Streaming/Introduction%20to%20Spark%20Streaming.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m ssc\u001b[39m.\u001b[39;49mawaitTermination()\n",
      "File \u001b[1;32mc:\\Users\\BAHA\\anaconda3\\lib\\site-packages\\pyspark\\streaming\\context.py:239\u001b[0m, in \u001b[0;36mStreamingContext.awaitTermination\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[39mWait for the execution to stop.\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39m    time to wait in seconds\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 239\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jssc\u001b[39m.\u001b[39;49mawaitTermination()\n\u001b[0;32m    240\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jssc\u001b[39m.\u001b[39mawaitTerminationOrTimeout(\u001b[39mint\u001b[39m(timeout \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\BAHA\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\BAHA\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o25.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"c:\\Users\\BAHA\\anaconda3\\lib\\site-packages\\pyspark\\streaming\\util.py\", line 71, in call\n    r = self.func(t, *rdds)\n  File \"c:\\Users\\BAHA\\anaconda3\\lib\\site-packages\\pyspark\\streaming\\dstream.py\", line 254, in takeAndPrint\n    taken = rdd.take(num + 1)\n  File \"c:\\Users\\BAHA\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 2836, in take\n    res = self.context.runJob(self, takeUpToNumLeft, p)\n  File \"c:\\Users\\BAHA\\anaconda3\\lib\\site-packages\\pyspark\\context.py\", line 2319, in runJob\n    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n  File \"c:\\Users\\BAHA\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n    return_value = get_return_value(\n  File \"c:\\Users\\BAHA\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 1) (DESKTOP-4JIO9JB.home executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 18 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\r\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\r\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\r\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\r\n\tat com.sun.proxy.$Proxy33.call(Unknown Source)\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\t... 3 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 18 more\r\n\n\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\r\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\r\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\r\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n"
     ]
    }
   ],
   "source": [
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Example\n",
    "In order to use all of this though, we need to setup a Developer API acocunt with Twitter and create an application to get credentials. Review the video for instructions on how to do this or if you are already familiar with it, just get the credentials from: \n",
    "\n",
    "    https://apps.twitter.com/\n",
    "    \n",
    "Once you have that you also need to install python-twitter, a python library to connect your Python to the twitter dev account.\n",
    "\n",
    "You probably won't be able to run this example and then previous in the same notebook, you need to restart you kernel.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "Begin by running the TweetRead.py file. Make sure to add your own IP Adress and your credential keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# May cause deprecation warnings, safe to ignore, they aren't errors\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Can only run this once. restart your kernel for any errors.\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10 )\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "socket_stream = ssc.socketTextStream(\"127.0.0.1\", 5555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = socket_stream.window( 20 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "fields = (\"tag\", \"count\" )\n",
    "Tweet = namedtuple( 'Tweet', fields )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use Parenthesis for multiple lines or use \\.\n",
    "( lines.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  .filter( lambda word: word.lower().startswith(\"#\") ) # Checks for hashtag calls\n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) # Reduces\n",
    "  .map( lambda rec: Tweet( rec[0], rec[1] ) ) # Stores in a Tweet Object\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") ) # Sorts Them in a DF\n",
    "  .limit(10).registerTempTable(\"tweets\") ) ) # Registers to a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "### Now run TweetRead.py\n",
    "__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssc.start()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Only works for Jupyter Notebooks!\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b6cf160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = 0\n",
    "while count < 10:\n",
    "    \n",
    "    time.sleep( 3 )\n",
    "    top_10_tweets = sqlContext.sql( 'Select tag, count from tweets' )\n",
    "    top_10_df = top_10_tweets.toPandas()\n",
    "    display.clear_output(wait=True)\n",
    "    sns.plt.figure( figsize = ( 10, 8 ) )\n",
    "    sns.barplot( x=\"count\", y=\"tag\", data=top_10_df)\n",
    "    sns.plt.show()\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
